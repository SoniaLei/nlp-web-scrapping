{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re, nltk\n",
    "from nltk.corpus import wordnet\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionaries for cleaning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first dictionary includes contractions and their associated expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Contraction_Dictionary1 = {\n",
    "    \"ain/t\": \"is not\", \"aren/t\": \"are not\", \"can/t\": \"can not\", \"can/t/ve\": \"can not have\", \"cause\": \"because\", \"could/ve\": \"could have\",\n",
    "    \"couldn/t\": \"could not\", \"couldn/t/ve\": \"could not have\", \"didn/t\": \"did not\", \"doesn/t\": \"does not\", \"don/t\": \"do not\", \"hadn/t\": \"had not\",\n",
    "    \"hadn/t/ve\": \"had not have\", \"hasn/t\": \"has not\", \"haven/t\": \"have not\", \"he/d\": \"he would\", \"he/d/ve\": \"he would have\", \"he/ll\": \"he will\",\n",
    "    \"he/ll/ve\": \"he he will have\", \"he/s\": \"he is\", \"how/d\": \"how did\", \"how/d/y\": \"how do you\", \"how/ll\": \"how will\", \"how/s\": \"how is\",\n",
    "    \"I/d\": \"I would\", \"I/d/ve\": \"I would have\", \"I/ll\": \"I will\", \"I/ll/ve\": \"I will have\", \"I/m\": \"I am\", \"I/ve\": \"I have\", \"i/d\": \"i would\",\n",
    "    \"i/d/ve\": \"i would have\", \"i/ll\": \"i will\", \"i/ll/ve\": \"i will have\", \"i/m\": \"i am\", \"i/ve\": \"i have\", \"isn/t\": \"is not\", \"it/d\": \"it would\",\n",
    "    \"it/d/ve\": \"it would have\", \"it/ll\": \"it will\", \"it/ll/ve\": \"it will have\", \"it/s\": \"it is\", \"let/s\": \"let us\", \"ma/am\": \"madam\", \"mayn/t\": \"may not\",\n",
    "    \"might/ve\": \"might have\", \"mightn/t\": \"might not\", \"mightn/t/ve\": \"might not have\", \"must/ve\": \"must have\", \"mustn/t\": \"must not\", \"mustn/t/ve\": \"must not have\",\n",
    "    \"needn/t\": \"need not\", \"needn/t/ve\": \"need not have\", \"o/clock\": \"of the clock\", \"oughtn/t\": \"ought not\", \"oughtn/t/ve\": \"ought not have\", \"shan/t\": \"shall not\",\n",
    "    \"sha/n/t\": \"shall not\", \"shan/t/ve\": \"shall not have\", \"she/d\": \"she would\", \"she/d/ve\": \"she would have\", \"she/ll\": \"she will\", \"she/ll/ve\": \"she will have\",\n",
    "    \"she/s\": \"she is\", \"should/ve\": \"should have\", \"shouldn/t\": \"should not\", \"shouldn/t/ve\": \"should not have\", \"so/ve\": \"so have\", \"so/s\": \"so as\",\n",
    "    \"that/d\": \"that would\", \"that/d/ve\": \"that would have\", \"that/s\": \"that is\", \"there/d\": \"there would\", \"there/d/ve\": \"there would have\",\n",
    "    \"there/s\": \"there is\", \"they/d\": \"they would\", \"they/d/ve\": \"they would have\", \"they/ll\": \"they will\", \"they/ll/ve\": \"they will have\", \"they/re\": \"they are\",\n",
    "    \"they/ve\": \"they have\", \"to/ve\": \"to have\", \"wasn/t\": \"was not\", \"we/d\": \"we would\", \"we/d/ve\": \"we would have\", \"we/ll\": \"we will\", \"we/ll/ve\": \"we will have\", \n",
    "    \"we/re\": \"we are\", \"we/ve\": \"we have\", \"weren/t\": \"were not\", \"what/ll\": \"what will\", \"what/ll/ve\": \"what will have\",\"what/re\": \"what are\", \"what/s\": \"what is\", \n",
    "    \"what/ve\": \"what have\", \"when/s\": \"when is\", \"when/ve\": \"when have\", \"where/d\": \"where did\", \"where/s\": \"where is\", \"where/ve\": \"where have\",\n",
    "    \"who/ll\": \"who will\", \"who/ll/ve\": \"who will have\", \"who/s\": \"who is\", \"who/ve\": \"who have\", \"why/s\": \"why is\", \"why/ve\": \"why have\", \"will/ve\": \"will have\", \n",
    "    \"won/t\": \"will not\",\"won/t/ve\": \"will not have\", \"would/ve\": \"would have\", \"wouldn/t\": \"would not\", \"wouldn/t/ve\": \"would not have\", \"y/all\": \"you all\",\n",
    "    \"y/all/d\": \"you all would\", \"y/all/d/ve\": \"you all would have\", \"y/all/re\": \"you all are\", \"y/all/ve\": \"you all have\", \"you/d\": \"you would\",\n",
    "    \"you/d/ve\": \"you would have\", \"you/ll\": \"you will\", \"you/ll/ve\": \"you will have\", \"you/re\": \"you are\", \"you/ve\": \"you have\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This list contains an edited list of stopwords, with all negation words (e.g. 'no', 'never', 'not') excluded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself',\n",
    "            'yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself',\n",
    "            'they','them','their','theirs','themselves','what','which','who','whom','this','that',\n",
    "            'these','those','am','is','are','was','were','be','been','being','have','has','had',\n",
    "            'having','do','does','did','doing','a','an','the','and','but','if','or','because','as',\n",
    "            'until','while','of','at','by','for','with','about','against','between','into','through',\n",
    "            'during','before','after','above','below','to','from','up','down','in','out','on','off',\n",
    "            'over','under','again','further','then','once','here','there','when','where','why','how',\n",
    "            'all','any','both','each','few','more','most','other','some','such',\n",
    "            'only','own','same','so','than','too','very','can','will','just','should',\n",
    "            'now','uses','use','using','used','one','also']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second list contains the nltk.wordnet labelling convertion for verbs, adjectives, nouns and adverbs. The purpose of this list is to only lemmatize words that are POS (part-of-speech) tagged with these labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PosList =[\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNS\",\"NNP\",\"NNPS\",\"RB\",\n",
    "          \"RBR\",\"RBS\",\"VB\",\"VBD\",\"VBG\",\"VBN\",\"VBP\",\"VBZ\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This second dictionary uses the POS tag label as a key to refer to the root/lemma of a word. The purpose of this is to identify words with these POS tags and lemmatize them to their root lemma. E.g. 'running' --> 'run'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Ernest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PosMapper = {\n",
    "\"JJ\": wordnet.ADJ,\n",
    "\"JJR\": wordnet.ADJ,\n",
    "\"JJS\": wordnet.ADJ,\n",
    "\"NN\": wordnet.NOUN,\n",
    "\"NNS\": wordnet.NOUN,\n",
    "\"NNP\": wordnet.NOUN,\n",
    "\"NNPS\": wordnet.NOUN,\n",
    "\"RB\": wordnet.ADV,\n",
    "\"RBR\": wordnet.ADV,\n",
    "\"RBS\": wordnet.ADV,\n",
    "\"VB\": wordnet.VERB,\n",
    "\"VBD\": wordnet.VERB,\n",
    "\"VBG\": wordnet.VERB,\n",
    "\"VBN\": wordnet.VERB,\n",
    "\"VBP\": wordnet.VERB,\n",
    "\"VBZ\": wordnet.VERB}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Innitialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text normalization/standardization method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method normalizes the text into a coherent format for matching\n",
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lower() # Convert to lowercase\n",
    "    df[text_field] = df[text_field].str.replace('http','') # removing urls is useful to make vocabulary small as possible\n",
    "    df[text_field] = df[text_field].str.replace(r'\\.com$', '') # removing .com from the end of a url - same as above\n",
    "    #df[text_field] = df[text_field].str.replace('com', '') # same as above.\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \" \")\n",
    "    #df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9()$,!?@\\`\\\"\\'\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9()$,!?@\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\") #  replacing at sign for a word\n",
    "    df[text_field] = df[text_field].str.replace(\".\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\",\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\"-\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\"(\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\")\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace('\"', \" \")\n",
    "    df[text_field] = df[text_field].str.replace(\"?\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(\"!\", \"\")\n",
    "    df[text_field] = df[text_field].str.strip(\"''\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contractions Expansion Prep\n",
    "In the data, contraction words such as wouldn't are noted as 'wouldn`t' ` which is a different character to the normal apostrophe. Therefore each instance is changed to a '/' in order to match contractions to the contraction dictionary equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method strips the ` and changes is to / in order to match contractions.\n",
    "def contractionPrep(df, text_field):\n",
    "    df[text_field] = df[text_field].str.lstrip(' ')\n",
    "    df[text_field] = df[text_field].str.replace(\"'\", '/')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contraction Expansion Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method expands all contractions to their original format\n",
    "def expand_contractions(text, contraction_mapping=Contraction_Dictionary1):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)                       \n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "        \n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    listOfTokens = []\n",
    "    for text in x:\n",
    "        text = str(text)\n",
    "        text = word_tokenize(text)\n",
    "        listOfTokens.append(text)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singleletter removal Method\n",
    "After an initial inspection into word frequency, single letter words were very frequent and didnt seem to contribute much semantic meaning the tweets, so were therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def singleLetterRemoval(list_object):\n",
    "    listOfTokens = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        for word in tweet:\n",
    "            if len(word) > 1:\n",
    "                temp.append(word)\n",
    "        listOfTokens.append(temp)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number Removal Method\n",
    "Similarly to single letters, numbers dont contribute much meaning to the polarity of a tweet and so therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberRemoval(list_object):\n",
    "    listOfTokens = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        for word in tweet:\n",
    "            if not word.isnumeric():\n",
    "                temp.append(word)\n",
    "        listOfTokens.append(temp)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword Removal Method\n",
    "Stopwords are the most frequent words in the corpus and only create noise for the classifier so were therefore removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwordRemoval(list_object):\n",
    "    listOfTokens = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        for word in tweet:\n",
    "            if not word in stop_words:\n",
    "                temp.append(word)\n",
    "        listOfTokens.append(temp)\n",
    "    return listOfTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization Method\n",
    "Calls the Pos list and dictionary to return certain words into their root lemma format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemma(list_object):\n",
    "    tags = []\n",
    "    for words in list_object:\n",
    "        posTupples = nltk.pos_tag(words)\n",
    "        text = [lemmatizer.lemmatize(k[0], pos=PosMapper.get(k[1])) if k[1] in PosList else k[0] for k in posTupples]\n",
    "        tags.append(text)\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append securities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendSecurities(list_object):\n",
    "    listOfSecurities = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        sentence = tweet.split()\n",
    "        for word in sentence:\n",
    "            if re.fullmatch(r'\\$[A-Z]{1,4}', word) and not re.fullmatch(r'\\$SPY', word) :\n",
    "                temp.append(word)\n",
    "        listOfSecurities.append(temp)\n",
    "    return listOfSecurities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove securities from content column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeSecurities(list_object):\n",
    "    sentence_without_dollar_signs = []\n",
    "    for tweet in list_object:\n",
    "        temp = []\n",
    "        sentence = tweet.split()\n",
    "        for word in sentence:\n",
    "            if not re.fullmatch(r'\\$[A-Z]{1,4}', word):\n",
    "                temp.append(word)\n",
    "        sentence_without_dollar_signs.append(' '.join(temp))\n",
    "    return sentence_without_dollar_signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove dollar signs from securities column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dollar_sign(securities):\n",
    "\n",
    "    without_dollar_sign = []\n",
    "    for security in securities:\n",
    "        tmp = []    \n",
    "        for s in security:\n",
    "            tmp.append(s.lstrip(\"$\"))\n",
    "        without_dollar_sign.append(tmp)\n",
    "    \n",
    "    return without_dollar_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in dataset and clean it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = path.split(path.split(os.getcwd())[0])[0]\n",
    "csv_filename = path.join(root_folder, 'data', 'raw', 'scrapedtweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "\n",
       "                                             content        date      time  \n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03  \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03  \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in data to clean\n",
    "data = pd.read_csv(csv_filename)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now remove securities from the dataset and add them to a new column\n",
    "\n",
    "Use remove securities function from earlier and apply to the content column in the dataset. Then output to a new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[$BA, $CCL, $RCL, $NCLH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[$SPX, $DJIA, $DIA, $QQQ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03   \n",
       "\n",
       "                  securities  \n",
       "0   [$BA, $CCL, $RCL, $NCLH]  \n",
       "1                         []  \n",
       "2  [$SPX, $DJIA, $DIA, $QQQ]  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#securities added to one column\n",
    "\n",
    "data['content'] = data['content'].astype('str')\n",
    "data['securities'] = appendSecurities(data['content'])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03   \n",
       "\n",
       "              securities  \n",
       "0   [BA, CCL, RCL, NCLH]  \n",
       "1                     []  \n",
       "2  [SPX, DJIA, DIA, QQQ]  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['securities'] = remove_dollar_sign(data['securities'])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03   \n",
       "\n",
       "              securities                                         tweet text  \n",
       "0   [BA, CCL, RCL, NCLH]                         travel going green bullish  \n",
       "1                     []                             letâ€™s go mooning today  \n",
       "2  [SPX, DJIA, DIA, QQQ]  Analysts on US stock markets: 1. On Monday, Bl...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the content column to remove any securities from the tweet and add it to a new column\n",
    "data['tweet text'] = removeSecurities(data['content'])\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>travel going green bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risksavage_inthemarket</td>\n",
       "      <td>/risksavage_inthemarket/message/226381105</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[GNLN, CGC, KERN, PM]</td>\n",
       "      <td>â€œWhat Does The Institutional Ownership Tell Us...</td>\n",
       "      <td>â€œWhat Does The Institutional Ownership Tell Us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user                                 message_id  \\\n",
       "0              babybounce              /babybounce/message/226382374   \n",
       "1              L1_Trading              /L1_Trading/message/226381562   \n",
       "2           Economist4401           /Economist4401/message/226381511   \n",
       "3                OkieOkie                /OkieOkie/message/226381256   \n",
       "4  risksavage_inthemarket  /risksavage_inthemarket/message/226381105   \n",
       "\n",
       "  sentiment                                            content        date  \\\n",
       "0   Bullish  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020   \n",
       "1   Bullish                        $SPY letâ€™s go mooning today  09/07/2020   \n",
       "2   Bearish  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020   \n",
       "3   Bearish  $SPY more China. China wants some of Australia...  09/07/2020   \n",
       "4   Bullish  $GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...  09/07/2020   \n",
       "\n",
       "       time             securities  \\\n",
       "0  12:21:03   [BA, CCL, RCL, NCLH]   \n",
       "1  12:21:03                     []   \n",
       "2  12:21:03  [SPX, DJIA, DIA, QQQ]   \n",
       "3  12:21:03                     []   \n",
       "4  12:21:03  [GNLN, CGC, KERN, PM]   \n",
       "\n",
       "                                          tweet text  \\\n",
       "0                         travel going green bullish   \n",
       "1                             letâ€™s go mooning today   \n",
       "2  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "4  â€œWhat Does The Institutional Ownership Tell Us...   \n",
       "\n",
       "                                              tokens  \n",
       "0                         travel going green bullish  \n",
       "1                             letâ€™s go mooning today  \n",
       "2  Analysts on US stock markets: 1. On Monday, Bl...  \n",
       "3  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜  \n",
       "4  â€œWhat Does The Institutional Ownership Tell Us...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['tokens'] = data['tweet text']\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>travel going green bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>let s go mooning today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>analysts on us stock markets  1  on monday  bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>more china  china wants some of australia lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risksavage_inthemarket</td>\n",
       "      <td>/risksavage_inthemarket/message/226381105</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[GNLN, CGC, KERN, PM]</td>\n",
       "      <td>â€œWhat Does The Institutional Ownership Tell Us...</td>\n",
       "      <td>what does the institutional ownership tell us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user                                 message_id  \\\n",
       "0              babybounce              /babybounce/message/226382374   \n",
       "1              L1_Trading              /L1_Trading/message/226381562   \n",
       "2           Economist4401           /Economist4401/message/226381511   \n",
       "3                OkieOkie                /OkieOkie/message/226381256   \n",
       "4  risksavage_inthemarket  /risksavage_inthemarket/message/226381105   \n",
       "\n",
       "  sentiment                                            content        date  \\\n",
       "0   Bullish  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020   \n",
       "1   Bullish                        $SPY letâ€™s go mooning today  09/07/2020   \n",
       "2   Bearish  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020   \n",
       "3   Bearish  $SPY more China. China wants some of Australia...  09/07/2020   \n",
       "4   Bullish  $GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...  09/07/2020   \n",
       "\n",
       "       time             securities  \\\n",
       "0  12:21:03   [BA, CCL, RCL, NCLH]   \n",
       "1  12:21:03                     []   \n",
       "2  12:21:03  [SPX, DJIA, DIA, QQQ]   \n",
       "3  12:21:03                     []   \n",
       "4  12:21:03  [GNLN, CGC, KERN, PM]   \n",
       "\n",
       "                                          tweet text  \\\n",
       "0                         travel going green bullish   \n",
       "1                             letâ€™s go mooning today   \n",
       "2  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "4  â€œWhat Does The Institutional Ownership Tell Us...   \n",
       "\n",
       "                                              tokens  \n",
       "0                         travel going green bullish  \n",
       "1                             let s go mooning today  \n",
       "2  analysts on us stock markets  1  on monday  bl...  \n",
       "3  more china  china wants some of australia lol      \n",
       "4   what does the institutional ownership tell us...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standardize Text\n",
    "\n",
    "data = standardize_text(data,'tokens')\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand contractions\n",
    "Only the dataset that is being cleaned calls these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data ready for Contraction Expansion\n",
    "data = contractionPrep(data,'tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>travel going green bullish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>let s go mooning today</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>analysts on us stock markets  1  on monday  bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>more china  china wants some of australia lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risksavage_inthemarket</td>\n",
       "      <td>/risksavage_inthemarket/message/226381105</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[GNLN, CGC, KERN, PM]</td>\n",
       "      <td>â€œWhat Does The Institutional Ownership Tell Us...</td>\n",
       "      <td>what does the institutional ownership tell us ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user                                 message_id  \\\n",
       "0              babybounce              /babybounce/message/226382374   \n",
       "1              L1_Trading              /L1_Trading/message/226381562   \n",
       "2           Economist4401           /Economist4401/message/226381511   \n",
       "3                OkieOkie                /OkieOkie/message/226381256   \n",
       "4  risksavage_inthemarket  /risksavage_inthemarket/message/226381105   \n",
       "\n",
       "  sentiment                                            content        date  \\\n",
       "0   Bullish  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020   \n",
       "1   Bullish                        $SPY letâ€™s go mooning today  09/07/2020   \n",
       "2   Bearish  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020   \n",
       "3   Bearish  $SPY more China. China wants some of Australia...  09/07/2020   \n",
       "4   Bullish  $GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...  09/07/2020   \n",
       "\n",
       "       time             securities  \\\n",
       "0  12:21:03   [BA, CCL, RCL, NCLH]   \n",
       "1  12:21:03                     []   \n",
       "2  12:21:03  [SPX, DJIA, DIA, QQQ]   \n",
       "3  12:21:03                     []   \n",
       "4  12:21:03  [GNLN, CGC, KERN, PM]   \n",
       "\n",
       "                                          tweet text  \\\n",
       "0                         travel going green bullish   \n",
       "1                             letâ€™s go mooning today   \n",
       "2  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "4  â€œWhat Does The Institutional Ownership Tell Us...   \n",
       "\n",
       "                                              tokens  \n",
       "0                         travel going green bullish  \n",
       "1                             let s go mooning today  \n",
       "2  analysts on us stock markets  1  on monday  bl...  \n",
       "3  more china  china wants some of australia lol      \n",
       "4  what does the institutional ownership tell us ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Expand Contractions\n",
    "cleanedData = [expand_contractions(str(tweet)) for tweet in data['tokens']]\n",
    "data['tokens'] = cleanedData\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inspecting the results there were still some square brackets remaining as part of some words so these needed to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#strip remaining / from data\n",
    "data['tokens'] = data['tokens'].str.replace('/', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize Data\n",
    "This is applied to both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "tweets = data['tokens'].tolist()\n",
    "tokenizedData = tokenizer(tweets)\n",
    "data['tokens'] = tokenizedData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>[travel, going, green, bullish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>[let, s, go, mooning, today]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         user                     message_id sentiment  \\\n",
       "0  babybounce  /babybounce/message/226382374   Bullish   \n",
       "1  L1_Trading  /L1_Trading/message/226381562   Bullish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "\n",
       "             securities                  tweet text  \\\n",
       "0  [BA, CCL, RCL, NCLH]  travel going green bullish   \n",
       "1                    []      letâ€™s go mooning today   \n",
       "\n",
       "                            tokens  \n",
       "0  [travel, going, green, bullish]  \n",
       "1     [let, s, go, mooning, today]  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call rest of cleaning methods on the dataset that is being cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>[travel, going, green, bullish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>[let, go, mooning, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>[analysts, on, us, stock, markets, on, monday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>[more, china, china, wants, some, of, australi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "3       OkieOkie       /OkieOkie/message/226381256   Bearish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03   \n",
       "3  $SPY more China. China wants some of Australia...  09/07/2020  12:21:03   \n",
       "\n",
       "              securities                                         tweet text  \\\n",
       "0   [BA, CCL, RCL, NCLH]                         travel going green bullish   \n",
       "1                     []                             letâ€™s go mooning today   \n",
       "2  [SPX, DJIA, DIA, QQQ]  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3                     []  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "\n",
       "                                              tokens  \n",
       "0                    [travel, going, green, bullish]  \n",
       "1                          [let, go, mooning, today]  \n",
       "2  [analysts, on, us, stock, markets, on, monday,...  \n",
       "3  [more, china, china, wants, some, of, australi...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#single letter removal:\n",
    "\n",
    "tweetData = data['tokens'].tolist()\n",
    "slRemoved = singleLetterRemoval(tweetData)\n",
    "data['tokens'] = slRemoved\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>[travel, going, green, bullish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>[let, go, mooning, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>[analysts, on, us, stock, markets, on, monday,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>[more, china, china, wants, some, of, australi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "3       OkieOkie       /OkieOkie/message/226381256   Bearish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03   \n",
       "3  $SPY more China. China wants some of Australia...  09/07/2020  12:21:03   \n",
       "\n",
       "              securities                                         tweet text  \\\n",
       "0   [BA, CCL, RCL, NCLH]                         travel going green bullish   \n",
       "1                     []                             letâ€™s go mooning today   \n",
       "2  [SPX, DJIA, DIA, QQQ]  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3                     []  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "\n",
       "                                              tokens  \n",
       "0                    [travel, going, green, bullish]  \n",
       "1                          [let, go, mooning, today]  \n",
       "2  [analysts, on, us, stock, markets, on, monday,...  \n",
       "3  [more, china, china, wants, some, of, australi...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number removal\n",
    "tweetData = data['tokens'].tolist()\n",
    "nRemoved = numberRemoval(tweetData)\n",
    "data['tokens'] = nRemoved\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_of_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>[travel, going, green, bullish]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>[let, go, mooning, today]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>[analysts, on, us, stock, markets, on, monday,...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>[more, china, china, wants, some, of, australi...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>risksavage_inthemarket</td>\n",
       "      <td>/risksavage_inthemarket/message/226381105</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[GNLN, CGC, KERN, PM]</td>\n",
       "      <td>â€œWhat Does The Institutional Ownership Tell Us...</td>\n",
       "      <td>[what, does, the, institutional, ownership, te...</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HeyYouWhoMe</td>\n",
       "      <td>/HeyYouWhoMe/message/226381022</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY up or down today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>up or down today</td>\n",
       "      <td>[up, or, down, today]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>KaroleinTriedToTrade</td>\n",
       "      <td>/KaroleinTriedToTrade/message/226380585</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY I hope this goes up so high</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>I hope this goes up so high</td>\n",
       "      <td>[hope, this, goes, up, so, high]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>DannETrader</td>\n",
       "      <td>/DannETrader/message/226380472</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY yesterday was last day of FED repoâ€™s. It ...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>yesterday was last day of FED repoâ€™s. It isnâ€™t...</td>\n",
       "      <td>[yesterday, was, last, day, of, fed, repo, it,...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>shoaibfatima</td>\n",
       "      <td>/shoaibfatima/message/226380359</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY get in before the pump starts</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>get in before the pump starts</td>\n",
       "      <td>[get, in, before, the, pump, starts]</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Burrrr_time</td>\n",
       "      <td>/Burrrr_time/message/226379857</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY fed buying stock !!! Ath</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>fed buying stock !!! Ath</td>\n",
       "      <td>[fed, buying, stock, ath]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     user                                 message_id  \\\n",
       "0              babybounce              /babybounce/message/226382374   \n",
       "1              L1_Trading              /L1_Trading/message/226381562   \n",
       "2           Economist4401           /Economist4401/message/226381511   \n",
       "3                OkieOkie                /OkieOkie/message/226381256   \n",
       "4  risksavage_inthemarket  /risksavage_inthemarket/message/226381105   \n",
       "5             HeyYouWhoMe             /HeyYouWhoMe/message/226381022   \n",
       "6    KaroleinTriedToTrade    /KaroleinTriedToTrade/message/226380585   \n",
       "7             DannETrader             /DannETrader/message/226380472   \n",
       "8            shoaibfatima            /shoaibfatima/message/226380359   \n",
       "9             Burrrr_time             /Burrrr_time/message/226379857   \n",
       "\n",
       "  sentiment                                            content        date  \\\n",
       "0   Bullish  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020   \n",
       "1   Bullish                        $SPY letâ€™s go mooning today  09/07/2020   \n",
       "2   Bearish  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020   \n",
       "3   Bearish  $SPY more China. China wants some of Australia...  09/07/2020   \n",
       "4   Bullish  $GNLN $CGC $SPY $KERN $PM â€œWhat Does The Insti...  09/07/2020   \n",
       "5   Bullish                              $SPY up or down today  09/07/2020   \n",
       "6   Bullish                   $SPY I hope this goes up so high  09/07/2020   \n",
       "7   Bearish  $SPY yesterday was last day of FED repoâ€™s. It ...  09/07/2020   \n",
       "8   Bullish                 $SPY get in before the pump starts  09/07/2020   \n",
       "9   Bullish                      $SPY fed buying stock !!! Ath  09/07/2020   \n",
       "\n",
       "       time             securities  \\\n",
       "0  12:21:03   [BA, CCL, RCL, NCLH]   \n",
       "1  12:21:03                     []   \n",
       "2  12:21:03  [SPX, DJIA, DIA, QQQ]   \n",
       "3  12:21:03                     []   \n",
       "4  12:21:03  [GNLN, CGC, KERN, PM]   \n",
       "5  12:21:03                     []   \n",
       "6  12:21:03                     []   \n",
       "7  12:21:03                     []   \n",
       "8  12:21:03                     []   \n",
       "9  12:21:03                     []   \n",
       "\n",
       "                                          tweet text  \\\n",
       "0                         travel going green bullish   \n",
       "1                             letâ€™s go mooning today   \n",
       "2  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "4  â€œWhat Does The Institutional Ownership Tell Us...   \n",
       "5                                   up or down today   \n",
       "6                        I hope this goes up so high   \n",
       "7  yesterday was last day of FED repoâ€™s. It isnâ€™t...   \n",
       "8                      get in before the pump starts   \n",
       "9                           fed buying stock !!! Ath   \n",
       "\n",
       "                                              tokens  num_of_tokens  \n",
       "0                    [travel, going, green, bullish]              4  \n",
       "1                          [let, go, mooning, today]              4  \n",
       "2  [analysts, on, us, stock, markets, on, monday,...             59  \n",
       "3  [more, china, china, wants, some, of, australi...              8  \n",
       "4  [what, does, the, institutional, ownership, te...             80  \n",
       "5                              [up, or, down, today]              4  \n",
       "6                   [hope, this, goes, up, so, high]              6  \n",
       "7  [yesterday, was, last, day, of, fed, repo, it,...             12  \n",
       "8               [get, in, before, the, pump, starts]              6  \n",
       "9                          [fed, buying, stock, ath]              4  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data['num_of_tokens'] = data['tokens'].apply(lambda x: len(x))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>message_id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>content</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>securities</th>\n",
       "      <th>tweet text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>num_of_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>babybounce</td>\n",
       "      <td>/babybounce/message/226382374</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$BA travel going green bullish $CCL $RCL $NCLH...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[BA, CCL, RCL, NCLH]</td>\n",
       "      <td>travel going green bullish</td>\n",
       "      <td>[travel, going, green, bullish]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1_Trading</td>\n",
       "      <td>/L1_Trading/message/226381562</td>\n",
       "      <td>Bullish</td>\n",
       "      <td>$SPY letâ€™s go mooning today</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>letâ€™s go mooning today</td>\n",
       "      <td>[let, go, mooning, today]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Economist4401</td>\n",
       "      <td>/Economist4401/message/226381511</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[SPX, DJIA, DIA, QQQ]</td>\n",
       "      <td>Analysts on US stock markets: 1. On Monday, Bl...</td>\n",
       "      <td>[analysts, us, stock, markets, monday, blackro...</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OkieOkie</td>\n",
       "      <td>/OkieOkie/message/226381256</td>\n",
       "      <td>Bearish</td>\n",
       "      <td>$SPY more China. China wants some of Australia...</td>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>12:21:03</td>\n",
       "      <td>[]</td>\n",
       "      <td>more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜</td>\n",
       "      <td>[china, china, wants, australia, lol]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            user                        message_id sentiment  \\\n",
       "0     babybounce     /babybounce/message/226382374   Bullish   \n",
       "1     L1_Trading     /L1_Trading/message/226381562   Bullish   \n",
       "2  Economist4401  /Economist4401/message/226381511   Bearish   \n",
       "3       OkieOkie       /OkieOkie/message/226381256   Bearish   \n",
       "\n",
       "                                             content        date      time  \\\n",
       "0  $BA travel going green bullish $CCL $RCL $NCLH...  09/07/2020  12:21:03   \n",
       "1                        $SPY letâ€™s go mooning today  09/07/2020  12:21:03   \n",
       "2  $SPY $SPX $DJIA $DIA $QQQ Analysts on US stock...  09/07/2020  12:21:03   \n",
       "3  $SPY more China. China wants some of Australia...  09/07/2020  12:21:03   \n",
       "\n",
       "              securities                                         tweet text  \\\n",
       "0   [BA, CCL, RCL, NCLH]                         travel going green bullish   \n",
       "1                     []                             letâ€™s go mooning today   \n",
       "2  [SPX, DJIA, DIA, QQQ]  Analysts on US stock markets: 1. On Monday, Bl...   \n",
       "3                     []  more China. China wants some of Australia lolðŸ¦˜ðŸ¦˜ðŸ¦˜ðŸ¦˜   \n",
       "\n",
       "                                              tokens  num_of_tokens  \n",
       "0                    [travel, going, green, bullish]              4  \n",
       "1                          [let, go, mooning, today]              4  \n",
       "2  [analysts, us, stock, markets, monday, blackro...             59  \n",
       "3              [china, china, wants, australia, lol]              8  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stopword removal\n",
    "tweetData = data['tokens'].tolist()\n",
    "noiseRemoved = stopwordRemoval(tweetData)\n",
    "data['tokens'] = noiseRemoved\n",
    "data.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize the cleaned Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Lemmatize Data\n",
    "tweetData = data['tokens'].tolist()\n",
    "lemmatizedData = lemma(tweetData)\n",
    "data['tokens_in_transformed_text'] = lemmatizedData\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.head(5) # cleanDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data['num_of_tokens_in_transformed_text'] = data['tokens_in_transformed_text'].apply(lambda x: len(x))\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export dataframes to csv files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export to CSV\n",
    "csv_out_filename = path.join(root_folder, 'data', 'raw', 'cleanedData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(csv_out_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
