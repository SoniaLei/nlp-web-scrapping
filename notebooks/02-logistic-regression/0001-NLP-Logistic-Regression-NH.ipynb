{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> Logisitic Regression Classification </center></h1>\n",
    "\n",
    "*https://realpython.com/logistic-regression-python/#multi-variate-logistic-regression*<br>\n",
    "*https://www.dataquest.io/blog/tutorial-text-classification-in-python-using-spacy/*\n",
    "\n",
    "## *Written by Nathanael Hitch*\n",
    "\n",
    "<hr>\n",
    "\n",
    "<span style=\"background-color:DeepPink; color:white; font-size:20px\">Appends Added:</span>\n",
    "\n",
    "1. Text_cleaner without using a spaCy model\n",
    "2. Model testing and training with different files\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is it?\n",
    "\n",
    "Classification refers to supervised machine learning that *tries* to predict which category an entity belongs in based on their features. In standard classification, these categories are discrete, finite possibilities, true/false, positive/negative.<br>\n",
    "Regression refers to continuos, unbounded outputs, e.g. estimating an employee's salary from features of their job.\n",
    "\n",
    "Logistic estimates the parameters between 2 outcomes, or more if the model needs to.\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "The classification is done by a *Sigmoid function*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\Sigmoids.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression's goal is to calculate its function such that its predicted outcome, $p(x_i)$, is as close as possible to the actual outcome. **Remember**, the actual response is, usually, a binary classification problem, e.g. 1 or 0. This means that each $p(x_i)$ should be close to either 0 or 1; hence why it’s convenient to use the sigmoid function.\n",
    "\n",
    "`Complex mathematically methodology:`<br>\n",
    "https://realpython.com/logistic-regression-python/#multi-variate-logistic-regression\n",
    "\n",
    "Using data where the actual outcome is know, the model will need to be trained/fitted and tested to show that it can accurately predict outcomes for new inputs. The training determines the best predicted weights for the predicited function, $p(x_i)$. Once determined, the predicted outcome = 1 when $p(x_i)$ > 0.5, and 0 otherwise. [The threshold hold value doesn't have to be 0.5 (usually is) and can be altered depending on the situation.]<br>\n",
    "\n",
    "## Classification Performance\n",
    "\n",
    "There are four possible types of results; as an example, a machine detecting whether a patient DOES (1) or DOES NOT (0) have cancer:\n",
    "\n",
    "1. **True Negative** = Correct Predicition: correctly predicted negatives (0)<br>\n",
    "Correctly predicted that the patient DOES NOT have cancer.\n",
    "\n",
    "2. **True Positive** = Correct Predicition: correclty predicted positives (1)<br>\n",
    "Correctly predicted that the patient DOES have cancer.\n",
    "\n",
    "3. **False Positive** = Incorrect Prediction: incorrectly predicted negatives (0)<br>\n",
    "Incorrectly predicted that the patient DOES have cancer when they DO NOT.\n",
    "\n",
    "4. **Fales Negative** = Incorrect Prediction: incorrectly predicted positives (1)<br>\n",
    "Incorrectly predicted that the patient DOES NOT have cancer when they DO.\n",
    "\n",
    "While the simplest indicator of the model's accuracy is the ratio of the number of correct predictions to the total number of predictions, there are other indicators of binary classifiers:\n",
    "\n",
    "- The positive/negative predictive value: ratio of the number of true positives/negatives to the sum of the numbers of true and false positives/negatives.\n",
    "- The sensitivity (recall or true positive rate): the ratio of the number of true positives to the number of actual positives.\n",
    "- The specificity (or true negative rate): ratio of the number of true negatives to the number of actual negatives.\n",
    "\n",
    "### Variates in Logisitic Regression\n",
    "\n",
    "**Single-variate** logisitic regression has one independent variable; one variable to make the predictive outcome on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\single-variate.png\" style=\"width:750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a given set of input-output (x-y) pairs, represented by green circles, that are your observations. Remember, the outcome is binary and can only be 0 or 1: e.g. for the leftmost green circle, x = 0 and the actual output y = 0. The rightmost observation has x = 9 and y = 1.\n",
    "\n",
    "Logistic regression finds the predicited weights, which find the the *logit*, $f(x)$, the dashed black line. This defines the predicted probability, $p(x) = 1 / (1 + exp(-f(x)))$, the full black line.<br>\n",
    "In the above case, the threshold $p(x)$ = 0.5 and $f(x)$ = 0 corresponds to $x$ being slightly higher than 3. This value is the limit between the inputs with the predicted outputs of 0 and 1.\n",
    "\n",
    "In the *real-world*, values where $x$ > 3 can have an actual outcome of 1 rather than the predicted value of 0 for this model.<br>\n",
    "This is where the Logisitic Regression model can be weak.\n",
    "\n",
    "**Multi-variate** logisitic regression has more than one input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images\\multi-variate.png\" style=\"width:750px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is different from the single-variate graph as both axes represent the inputs. The outputs differ in color: white circles = 0, the green circles = 1.\n",
    "\n",
    "For multi-variate function, there would be more predicted weights required for the logit, and the predicited probability for the logisitic function, $p(x_1,x_2) = 1 / (1 + exp(-f(x_1,x_2)))$.<br>\n",
    "The dash-dotted black line (i.e. the logit) linearly separates the two classes with the line corresponding to $p(x_1,x_2)$ = 0.5 and $f(x_1,x_2)$ = 0.\n",
    "\n",
    "### Overfit & Regularisation\n",
    "\n",
    "Overfitting occurs when a model learns the training data too well; the model learns not only the relationships among data but also the noise in the dataset.<br>\n",
    "Overfitting usually occurs with complex models; models tend to perform well with data used to fit them, the training data, while performing poorly with unseen data, or test data.\n",
    "\n",
    "Regularisation can significantly improve model performance on unseen data. This is done by reducing the complexity of the model, with techniques applied with logistic regression mostly tending to penalize large predicted weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example\n",
    "***\n",
    "This model will look at the review text of Amazon products and predict whether it is positive or negative.\n",
    "***\n",
    "\n",
    "The information is a tab-separated-value file (.tsv) with 5 columns:\n",
    "\n",
    "- Rating: rating each user gave the Alexa (out of 5)\n",
    "- Date: date of the review\n",
    "- Variation: the model the user is reviewing\n",
    "- Verified_Review: text of each review\n",
    "- Feedback: contains a sentiment label; 1 = positive, 2 = negative\n",
    "\n",
    "The feedback column already includes whether the review was positive or negative, we can use that to test the model.\n",
    "\n",
    "Start with uploading the necessary data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating       date         variation  \\\n",
      "0       5  31-Jul-18  Charcoal Fabric    \n",
      "1       5  31-Jul-18  Charcoal Fabric    \n",
      "2       4  31-Jul-18    Walnut Finish    \n",
      "3       5  31-Jul-18  Charcoal Fabric    \n",
      "4       5  31-Jul-18  Charcoal Fabric    \n",
      "\n",
      "                                    verified_reviews  feedback  \n",
      "0                                      Love my Echo!         1  \n",
      "1                                          Loved it!         1  \n",
      "2  Sometimes while playing a game, you can answer...         1  \n",
      "3  I have had a lot of fun with this thing. My 4 ...         1  \n",
      "4                                              Music         1   \n",
      "\n",
      "(3150, 5) \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3150 entries, 0 to 3149\n",
      "Data columns (total 5 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   rating            3150 non-null   int64 \n",
      " 1   date              3150 non-null   object\n",
      " 2   variation         3150 non-null   object\n",
      " 3   verified_reviews  3150 non-null   object\n",
      " 4   feedback          3150 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 123.2+ KB\n",
      "None \n",
      "\n",
      "1    2893\n",
      "0     257\n",
      "Name: feedback, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Importing needed packages\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "df_amazon = pd.read_csv(\"Files/amazon_alexa.tsv\", sep=\"\\t\")\n",
    "# Loading the Amazon .tsv file.\n",
    "    # 'sep' is the delimeter to use - can't automatically detect the separator\n",
    "    # '\\t' = tab\n",
    "    \n",
    "\"\"\" Useful DataFrame (df) functions \"\"\"\n",
    "\n",
    "print(df_amazon.head(),\"\\n\")\n",
    "# The first 5 records from the DataFrame\n",
    "\n",
    "print(df_amazon.shape,\"\\n\")\n",
    "# Returns a tuple of the dimensions of the DataFrame\n",
    "\n",
    "print(df_amazon.info(),\"\\n\")\n",
    "# View data information\n",
    "\n",
    "print(df_amazon.feedback.value_counts())\n",
    "# Values of each feedback option (i.e. 1 or 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use spaCy to tokenise the data, strip information we don't need (stopwords, punctuation etc.), perform Lemmatisation and lowercaste the text.\n",
    "\n",
    "`Print's have been commented out for when creating the Logistic Regression Model at the end.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'sentence', 'test', 'test', 'London']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string # Contains a useful list of punctuation marks.\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Create list of punctuation marks\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    #print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    #print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "       \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    #print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    #print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "    \n",
    "spacy_cleaner(\"This is not a test sentence, for testing tests from London.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further clean our text data we will create a **custom transformer**, removing end spaces and converting text into lower case. Here, we will create a custom predictors class wich inherits the TransformerMixin class(used to transform data/text). This class overrides the transform, fit and get_parrams methods. We’ll also create a clean_text() function that removes spaces and converts text into lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer using spaCy\n",
    "\n",
    "from sklearn.base import TransformerMixin\n",
    "class predictors(TransformerMixin):\n",
    "# Inheriting from TransformerMixin\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "    # fit: used for training your model without any pre-processing on the data    \n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify text in positive or negative labels is called Sentiment Analysis; to do that we need to represent our text numerically.<br>\n",
    "We can, amongst other ways, use the Bag-of-Words model to convert the text into a matrix of how many times a word has occured.\n",
    "\n",
    "We can generate a BoW matrix by using *Scikit-Learn*'s **CountVectoriser**. We will tell the CountVectoriser to use our function *spacy_cleaner* as its tokeniser, and define the n-gram range. For this one, we will be using uni-grams (one words), and the ngrams will be assigned to 'bow_vector'.<br>\n",
    "It would be good to look at the TF-IDF; this can be generated by using the *Scikit-Learn*'s **TfidfVectoriser**, with the tokeniser our *spacy_cleaner* function and the result assigned to 'tfidf_vector':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "#bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "bow_vector = CountVectorizer(ngram_range=(1,1))\n",
    "# Bag-of-Words n-gram matrix\n",
    "\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = spacy_cleaner)\n",
    "# TF-IDF result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data between **'Training'** and **'Test'** sets\n",
    "\n",
    "We will use half the data set as a **training** set, and the other half as a **testing** set.<br>\n",
    "Fortunately, *scikit-learn* has a built in function for doing this, **train_test_split()**:\n",
    "\n",
    "- X = what we want to split\n",
    "- ylabels = labels we want to test aganist\n",
    "- Plus the size of the test set, in percentage form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Splitting between Training and Testing sets \"\"\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# df_amazon is the .tsv file that has been previously loaded:\n",
    "    # df_amazon = pd.read_csv(\"Files/amazon_alexa.tsv\", sep=\"\\t\")\n",
    "\n",
    "X = df_amazon['verified_reviews'] # 'verified_reviews' is what we want to analyse\n",
    "\n",
    "ylabels = df_amazon['feedback'] # 'feedback' is the label/answer to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3)\n",
    "# Appling X, ylabels and the test size as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the **Logistice Regression Model**\n",
    "\n",
    "Start by importing the LogisiticRegression module; then create a LogisticRegression classifier object. \n",
    "\n",
    "*For this example*, we will build a pipeline with 3 components:\n",
    "\n",
    "- Classifier: clean and preprocess the text\n",
    "- Vectoriser: creates a BoW matrix for our text\n",
    "- Classifier: performs a logisitc regression to classify sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('cleaner', <__main__.predictors object at 0x000002068601CBC8>),\n",
       "                ('vectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "# Creating the LogisticRegression classifier object\n",
    "\n",
    "# Create a pipeline:\n",
    "    # These will using previously made function\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector), # Bag-of-Words CountVectoriser\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "pipe.fit(X_train,y_train)\n",
    "# Generating/training the model using the previously stated training setsa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Let's see how our model actually performs not that it has been trained.<br>\n",
    "This is done by using the *metrics* module; we will put our test data through the pipeline to come up with predictions and use the various functions from the metrics module to look at aspects of the pipeline:\n",
    "\n",
    "- Accuracy: percentage of the total predictions that are completely correct.\n",
    "- Precision: ratio of true positives plus false positive.\n",
    "- Recall: ratio of true positives to true positives plus false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9365079365079365\n",
      "Logistic Regression Precision: 0.9415656008820287\n",
      "Logistic Regression Recall: 0.991869918699187\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted)) # Accuracy\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted)) # Precision\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted)) # Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, the model correctly identified the comment's sentiment **94.1%** of the time.<br>\n",
    "When a review was predicted as positive, it was positive **95%** of the time<br>\n",
    "When given a positive review, the model identified it as positive **98.6%** of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification - One Vs. Rest (OVR)\n",
    "\n",
    "The standard Logistic Regression model is for use in binary classification. In the case of more than 2 classes, a heuristic method is needed to classify the data. There are 2 methods:\n",
    "\n",
    "- One-vs-Rest (OVR)\n",
    "- One-vs-One (OVO)\n",
    "\n",
    "Both methods split the multi-class dataset into separate binary classification problems. **One-vs-Rest** splits the problems into one possibility versus the rest; in the scenario used for the NLP project, their are 3 classifications, Positive, Neutral and Negative. Hence **OVR** splits the problems:\n",
    "\n",
    "1. Positive vs [Neutral, Negative]\n",
    "2. Neutral vs [Positive, Negative]\n",
    "3. Negative vs [Positive, Neutral]\n",
    "\n",
    "**One-vs-One** splits the problems into classifications aganist other individual classifications:\n",
    "\n",
    "1. Positive vs Neutral\n",
    "2. Positive vs Negative\n",
    "3. Neutral vs Negative\n",
    "\n",
    "In datasets with more than 3 classifications, this leads to far more binary problems.<br>\n",
    "We will be looking at OVR as OVO this approch is primarily suggested for Support Vector Machines (SVM).\n",
    "\n",
    "The code for OVR is very similar to a standard Logisitic Regression. The **difference** comes when declaring the classifier; the *OneVsRestClassifier* needs to be imported from sklearn, then declared with a Logisitic Regression classifier injected into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_estimator_type', '_first_estimator', '_get_param_names', '_get_tags', '_more_tags', '_pairwise', '_required_parameters', 'coef_', 'decision_function', 'estimator', 'fit', 'get_params', 'intercept_', 'multilabel_', 'n_classes_', 'n_jobs', 'partial_fit', 'predict', 'predict_proba', 'score', 'set_params']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "LogReg = LogisticRegression()\n",
    "# A Logisitic Regression object needs to be injected into OvR\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "\n",
    "print(dir(ovr))\n",
    "# Lists the classes Attributes and Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is from the NLP project with the same classification possibilities.\n",
    "\n",
    "`Code below taken from LR-OVR_Model-NH_MT.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import string\n",
    "import spacy\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "    \n",
    "################################################################################################################\n",
    "                ##### Filepaths will need to be changed #####\n",
    "################################################################################################################\n",
    "\n",
    "df_train = pd.read_csv(\"Files/raw/test/tweets-test_1.csv\")\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Create list of punctuation marks\n",
    "\n",
    "def spacy_cleaner(sentence):\n",
    "    \n",
    "    #print(\"Input sentence:\\n\", sentence,\"\\n\")\n",
    "    \n",
    "    doc = nlp(sentence.strip())\n",
    "    # Pass text into model's pipeline.\n",
    "    \n",
    "    myTokens = [token for token in doc]\n",
    "    # Creating a list of the words in the sentence.\n",
    "    #print(\"Sentence tokenised:\\n\", myTokens,\"\\n\")\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token.is_stop == False and token.text not in punctuations]\n",
    "    # List of words without stopwords or punctuations.\n",
    "    #print(\"Sentence without stopwords or punctuations:\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    myTokens = [token.lemma_.strip().lower() if token.pos_ != \"PROPN\" else token.lemma_.strip() \\\n",
    "                for token in myTokens]\n",
    "    # Words are lemmatised, spaces at end removed and (if not a proper noun) lowercased.\n",
    "    \n",
    "    myTokens = [token for token in myTokens if token != \"\"]\n",
    "    \n",
    "    #print(\"Sentence lemmatisted, no spaces and lowercase (except Proper Noun):\\n\", myTokens, \"\\n\")\n",
    "    \n",
    "    return myTokens\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = spacy_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Splitting into Training and Testing sets\n",
    "\n",
    "X_tr = df_train['text']\n",
    "Y_tr = df_train['sentiment']\n",
    "\n",
    "# Below needed if splitting one .csv file into training and testing sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tr, Y_tr, test_size=0.3)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "    \n",
    "LogReg = LogisticRegression()\n",
    "# Logisitic Regression classifier declared\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "# One-vs-Rest classifier declared with 'LogReg' injected\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"RAW DATA - BoW:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Logistic Regression F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>APPEND 1</center></h1>\n",
    "\n",
    "# Text_cleaner without using a spaCy model\n",
    "\n",
    "As mentioned, the text_cleaner developed for the Logistic Regression model uses a spaCy model. These have their advantages as they return tokenised objects which have methods that can be used directly, e.g. Lemmatisation (*lemma_*) or Stop Words (*is_stop*). On the other hand, NLTK returns string objects that have been affected by NLTK classes, like a Lemmatiser.<br>\n",
    "**However**, the spaCy model can slow down the model as the model is applied to each string that comes from the file.\n",
    "\n",
    "While it doesn't slow down the Logistic Regression model, using the spaCy model on the 'Random Forest' classifier drastically slowed it down.<br>\n",
    "Below is a code for tokenising and cleaning the text the same way as the 'spacy_cleaner' in this notebook, but without using a spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import string\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import nltk\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>APPEND 2</center></h1>\n",
    "\n",
    "# Model testing and training with different files\n",
    "\n",
    "You can use different files to train and then test the model. In order to do this, when initialising the variable for the dataframe's column, you need to caste them as type 'string'.<br>\n",
    "This appears to be something that *train_test_split* does automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                        # Importing needed packages\n",
    "    \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import winsound\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Reading .csv file\n",
    "\n",
    "df_train = pd.read_csv(\"Files/raw/tweets-train.csv\")\n",
    "\n",
    "df_test = pd.read_csv(\"Files/raw/tweets-test.csv\")\n",
    "\n",
    "X_train = df_train['text'].astype(str)\n",
    "Y_train = df_train['sentiment'].astype(str)\n",
    "\n",
    "X_test = df_test['text'].astype(str)\n",
    "Y_test = df_test['sentiment'].astype(str)\n",
    "\n",
    "# The X/Y_test/train functions ensured \n",
    "    # Otherwise the .fit() function would come up with an error\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating custom cleaner function\n",
    "\n",
    "Lemmatiser = nltk.stem.WordNetLemmatizer()\n",
    "# Instantiating the NLTK Lemmatiser\n",
    "\n",
    "punctuations = string.punctuation\n",
    "# Putting punctuation symbols into an object\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Import spacy model\n",
    "\n",
    "stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "# A list of stopwords that can be filtered out\n",
    "    # NLTK also has a stop words object but it has fewer words\n",
    "\n",
    "def text_cleaner(sentence):    \n",
    "                \n",
    "    sentence = \"\".join([char for char in sentence.strip() if char not in punctuations])\n",
    "    # Getting rid of any punctuation characters\n",
    "    \n",
    "    myTokens = re.split('\\W+', sentence)\n",
    "    # Tokenising the words\n",
    "    \n",
    "    myTokens = [token.lower() for token in myTokens if token not in stopwords]\n",
    "    # Removing stop words\n",
    "    \n",
    "    myTokens = [Lemmatiser.lemmatize(token) for token in myTokens]\n",
    "    # Lemmatising the words and putting in lower case except for proper nouns\n",
    "    \n",
    "    return myTokens \n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Creating Bag-of-Words Vectoriser\n",
    "\n",
    "bow_vector = CountVectorizer(tokenizer = text_cleaner, ngram_range=(1,1))\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Building OVR Logisitic Regression Classifier\n",
    "\n",
    "LogReg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "ovr = OneVsRestClassifier(LogReg)\n",
    "\n",
    "pipe = Pipeline([('vectorizer', bow_vector)\n",
    "                 ,('classifier', ovr)])\n",
    "\n",
    "pipe.fit(X_train, Y_train)\n",
    "\n",
    "#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-#-\n",
    "\n",
    "                        # Evaluating the model\n",
    "\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "print(\"RAW DATA - BoW:\\n\")\n",
    "print(\"Logistic Regression Accuracy:\\n\",metrics.accuracy_score(Y_test, predicted),\"\\n\") # Accuracy\n",
    "print(\"Logistic Regression Precision:\\n\",metrics.precision_score(Y_test, predicted, average='macro'),\"\\n\") # Precision\n",
    "print(\"Logistic Regression Recall:\\n\",metrics.recall_score(Y_test, predicted, average='macro'),\"\\n\") # Recall\n",
    "print(\"Logistic Regression F1 Score:\\n\",metrics.f1_score(Y_test, predicted, average='macro')) # F1 Score\n",
    "\n",
    "winsound.PlaySound(\"Files/Alarm07.wav\", winsound.SND_FILENAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
